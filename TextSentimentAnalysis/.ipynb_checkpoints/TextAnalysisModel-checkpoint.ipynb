{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\HP\\\\Desktop\\\\ProjectSem6\\\\VSC\\\\sentiment-analysis\\\\TextSentimentAnalysis'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import speech_recognition as sr   \n",
    "import time \n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv('text_emotion.csv',usecols=['sentiment','content'],delimiter=\",\")\n",
    "data=pd.read_csv('tweet_emotions1.csv',delimiter=\",\")\n",
    "# shuffle dataset\n",
    "# data=data.sample(frac=1)\n",
    "#In a variation on the popular task of sentiment analysis, this dataset contains labels for the emotional content \n",
    "#(such as happiness, sadness, and anger) of texts. Hundreds to thousands of examples across 13 labels.\n",
    "#A subset of this data is used in an experiment we uploaded to Microsoft's Cortana Intelligence Gallery. \n",
    "#Added: July 15, 2016 by CrowdFlower | Data Rows: 40000 Download Now\n",
    "#Source: https://www.crowdflower.com/data-for-everyone/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness      11430\n",
      "neutral       9644\n",
      "happy         9564\n",
      "worry         8459\n",
      "love          5482\n",
      "happiness     5209\n",
      "anger         4426\n",
      "surprise      3066\n",
      "fear          2652\n",
      "relief        1526\n",
      "Name: Emotion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['Emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count={\n",
    "#     'happy':0,\n",
    "#     'sadness':0,\n",
    "#     'anger':0,\n",
    "#     'fear':0,\n",
    "#     'love':0,\n",
    "#     'surprise':1700\n",
    "# }\n",
    "# content=[]\n",
    "# sentiment=[]\n",
    "# for i in range(0,len(data['Emotion'])):\n",
    "#     if(count[data['Emotion'][i]] < 1700):\n",
    "#         content.append(data['Text'][i])\n",
    "#         sentiment.append(data['Emotion'][i])\n",
    "#         count[data['Emotion'][i]]+=1\n",
    "# df = pd.DataFrame()\n",
    "# df['content']=content\n",
    "# df['sentiment']=sentiment\n",
    "# df.head()\n",
    "# df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0  neutral  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  sadness                Funeral ceremony...gloomy friday...\n",
       "3    happy               wants to hang out with friends SOON!\n",
       "4  neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>happy</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text sentiment  \\\n",
       "0  neutral  @tiffanylue i know  i was listenin to bad habi...   neutral   \n",
       "1  sadness  Layin n bed with a headache  ughhhh...waitin o...   sadness   \n",
       "2  sadness                Funeral ceremony...gloomy friday...   sadness   \n",
       "3    happy               wants to hang out with friends SOON!     happy   \n",
       "4  neutral  @dannycastillo We want to trade with someone w...   neutral   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data['sentiment']=data['Emotion']\n",
    "data['sentiment']=data['Emotion']\n",
    "data['content']=data['Text']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'sadness', 'happy', 'worry', 'surprise', 'love',\n",
       "       'anger', 'happiness', 'relief', 'fear'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61458, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using re to remove twitter handles from content\n",
    "def remove_pattern(input_txt,pattern):\n",
    "    r=re.findall(pattern,input_txt)\n",
    "    for word in r:\n",
    "        input_txt=re.sub(word,\"\",input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing handles\n",
    "data['clean_content']=np.vectorize(remove_pattern)(data['content'],\"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        neutral\n",
       "1        sadness\n",
       "2        sadness\n",
       "3          happy\n",
       "4        neutral\n",
       "          ...   \n",
       "61453       fear\n",
       "61454       fear\n",
       "61455       fear\n",
       "61456       fear\n",
       "61457       fear\n",
       "Name: Emotion, Length: 61458, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>happy</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>We want to trade with someone who has Houston...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text sentiment  \\\n",
       "0  neutral  @tiffanylue i know  i was listenin to bad habi...   neutral   \n",
       "1  sadness  Layin n bed with a headache  ughhhh...waitin o...   sadness   \n",
       "2  sadness                Funeral ceremony...gloomy friday...   sadness   \n",
       "3    happy               wants to hang out with friends SOON!     happy   \n",
       "4  neutral  @dannycastillo We want to trade with someone w...   neutral   \n",
       "\n",
       "                                             content  \\\n",
       "0  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2                Funeral ceremony...gloomy friday...   \n",
       "3               wants to hang out with friends SOON!   \n",
       "4  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                       clean_content  \n",
       "0   i know  i was listenin to bad habit earlier a...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4   We want to trade with someone who has Houston...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "#data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Emotion                                               Text sentiment  \\\n",
      "0  neutral  @tiffanylue i know  i was listenin to bad habi...   neutral   \n",
      "1  sadness  Layin n bed with a headache  ughhhh...waitin o...   sadness   \n",
      "2  sadness                Funeral ceremony...gloomy friday...   sadness   \n",
      "3    happy               wants to hang out with friends SOON!     happy   \n",
      "4  neutral  @dannycastillo We want to trade with someone w...   neutral   \n",
      "\n",
      "                                             content  \\\n",
      "0  @tiffanylue i know  i was listenin to bad habi...   \n",
      "1  Layin n bed with a headache  ughhhh...waitin o...   \n",
      "2                Funeral ceremony...gloomy friday...   \n",
      "3               wants to hang out with friends SOON!   \n",
      "4  @dannycastillo We want to trade with someone w...   \n",
      "\n",
      "                                       clean_content  \n",
      "0   i know  i was listenin to bad habit earlier a...  \n",
      "1  Layin n bed with a headache  ughhhh   waitin o...  \n",
      "2                Funeral ceremony   gloomy friday     \n",
      "3               wants to hang out with friends SOON   \n",
      "4   We want to trade with someone who has Houston...  \n"
     ]
    }
   ],
   "source": [
    "# remove special char, numbers and punctuations\n",
    "data['clean_content']=data['clean_content'].str.replace(\"[^a-zA-Z#]\",\" \")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh   waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony   gloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>wants to hang out with friends SOON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>We want to trade with someone who has Houston...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                      clean_content\n",
       "0   neutral   i know  i was listenin to bad habit earlier a...\n",
       "1   sadness  Layin n bed with a headache  ughhhh   waitin o...\n",
       "2   sadness                Funeral ceremony   gloomy friday   \n",
       "3     happy               wants to hang out with friends SOON \n",
       "4   neutral   We want to trade with someone who has Houston..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dropping the content column\n",
    "data.drop('content',axis=1,inplace=True)\n",
    "data.drop('Text',axis=1,inplace=True)\n",
    "data.drop('Emotion',axis=1,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['happy'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['sadness'],\n",
       " ['love'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['happy'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['happiness'],\n",
       " ['happy'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['happy'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['happiness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['happiness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['love'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['anger'],\n",
       " ['love'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['sadness'],\n",
       " ['happiness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['love'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['relief'],\n",
       " ['happy'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['love'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['happy'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['happy'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['happy'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['surprise'],\n",
       " ['surprise'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['happy'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['anger'],\n",
       " ['love'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['relief'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['surprise'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['happiness'],\n",
       " ['happiness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['happiness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['happiness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['surprise'],\n",
       " ['surprise'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['surprise'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['happy'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['surprise'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['happiness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['happiness'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['relief'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['happiness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['surprise'],\n",
       " ['relief'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['anger'],\n",
       " ['happiness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['surprise'],\n",
       " ['worry'],\n",
       " ['surprise'],\n",
       " ['happiness'],\n",
       " ['sadness'],\n",
       " ['love'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['relief'],\n",
       " ['worry'],\n",
       " ['love'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['relief'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['happy'],\n",
       " ['neutral'],\n",
       " ['happy'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['love'],\n",
       " ['love'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['happy'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['surprise'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['happiness'],\n",
       " ['surprise'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['love'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['happy'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['love'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['surprise'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['happiness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['happiness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['relief'],\n",
       " ['happiness'],\n",
       " ['neutral'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['happiness'],\n",
       " ['happy'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['anger'],\n",
       " ['surprise'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['surprise'],\n",
       " ['neutral'],\n",
       " ['happy'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['happiness'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['happiness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['surprise'],\n",
       " ['happy'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['surprise'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['love'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['relief'],\n",
       " ['happiness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['happiness'],\n",
       " ['sadness'],\n",
       " ['relief'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['surprise'],\n",
       " ['neutral'],\n",
       " ['surprise'],\n",
       " ['neutral'],\n",
       " ['love'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['happy'],\n",
       " ['neutral'],\n",
       " ['love'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['surprise'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['happiness'],\n",
       " ['sadness'],\n",
       " ['relief'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['love'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['happy'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['love'],\n",
       " ['anger'],\n",
       " ['happiness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['love'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['happiness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['happiness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['surprise'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['surprise'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['happy'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['love'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['happy'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['happy'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['happy'],\n",
       " ['neutral'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['love'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['relief'],\n",
       " ['anger'],\n",
       " ['love'],\n",
       " ['surprise'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['love'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['happiness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['happy'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['happy'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['happy'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['anger'],\n",
       " ['surprise'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['love'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['surprise'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['surprise'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['surprise'],\n",
       " ['love'],\n",
       " ['neutral'],\n",
       " ['love'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['surprise'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['happy'],\n",
       " ['anger'],\n",
       " ['surprise'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['love'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['happiness'],\n",
       " ['anger'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['happy'],\n",
       " ['happy'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['love'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['sadness'],\n",
       " ['anger'],\n",
       " ['surprise'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['love'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['neutral'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['love'],\n",
       " ['sadness'],\n",
       " ['surprise'],\n",
       " ['happiness'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['surprise'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['worry'],\n",
       " ['sadness'],\n",
       " ['worry'],\n",
       " ['happy'],\n",
       " ['anger'],\n",
       " ['neutral'],\n",
       " ['neutral'],\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments=data['sentiment']\n",
    "sentiments\n",
    "sentiments_encoded=[] \n",
    "for sentiment in sentiments:\n",
    "    temp=[]\n",
    "    temp.append(sentiment)\n",
    "    sentiments_encoded.append(temp)\n",
    "sentiments_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i know  i was listenin to bad habit earlier and i started freakin at his part   \n",
      "['anger' 'fear' 'happiness' 'happy' 'love' 'neutral' 'relief' 'sadness'\n",
      " 'surprise' 'worry']\n",
      "[0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "tag_encoder = MultiLabelBinarizer()\n",
    "tags_encoded = tag_encoder.fit_transform(sentiments_encoded)\n",
    "num_tags = len(tags_encoded[0])\n",
    "print(data['clean_content'].values[0])\n",
    "print(tag_encoder.classes_)\n",
    "print(tags_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 55312\n",
      "Test size: 6146\n"
     ]
    }
   ],
   "source": [
    "# Split our data into train and test sets\n",
    "train_size = int(len(data) * .9)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(data) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split labels into train and test set\n",
    "train_tags=tags_encoded[:train_size]\n",
    "test_tags=tags_encoded[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "class TextPreprocessor(object):\n",
    "  def __init__(self, vocab_size):\n",
    "    self._vocab_size = vocab_size\n",
    "    self._tokenizer = None\n",
    "  \n",
    "  def create_tokenizer(self, text_list):\n",
    "    tokenizer = text.Tokenizer(num_words=self._vocab_size)\n",
    "    tokenizer.fit_on_texts(text_list)\n",
    "    self._tokenizer = tokenizer\n",
    "\n",
    "  def transform_text(self, text_list):\n",
    "    text_matrix = self._tokenizer.texts_to_matrix(text_list)\n",
    "    return text_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.71 GiB for an array with shape (55312, 9000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-64a9e4f82e5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_qs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mbody_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_qs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mbody_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_qs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-01b9754f7cf2>\u001b[0m in \u001b[0;36mtransform_text\u001b[1;34m(self, text_list)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtransform_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtext_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras_preprocessing\\text.py\u001b[0m in \u001b[0;36mtexts_to_matrix\u001b[1;34m(self, texts, mode)\u001b[0m\n\u001b[0;32m    381\u001b[0m         \"\"\"\n\u001b[0;32m    382\u001b[0m         \u001b[0msequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequences_to_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msequences_to_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras_preprocessing\\text.py\u001b[0m in \u001b[0;36msequences_to_matrix\u001b[1;34m(self, sequences, mode)\u001b[0m\n\u001b[0;32m    411\u001b[0m                              'before using tfidf mode.')\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.71 GiB for an array with shape (55312, 9000) and data type float64"
     ]
    }
   ],
   "source": [
    "# Create bow\n",
    "# Create vocab from training corpus \n",
    "\n",
    "VOCAB_SIZE=9000 # This is a hyperparameter, try out different values for your dataset\n",
    "\n",
    "train_qs = data['clean_content'].values[:train_size]\n",
    "test_qs = data['clean_content'].values[train_size:]\n",
    "\n",
    "processor = TextPreprocessor(VOCAB_SIZE)\n",
    "processor.create_tokenizer(train_qs)\n",
    "\n",
    "body_train = processor.transform_text(train_qs)\n",
    "body_test = processor.transform_text(test_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(body_train[0]))\n",
    "print(body_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer as it will be used in deployment\n",
    "import pickle\n",
    "with open('./processor_state.pkl','wb') as f:\n",
    "  pickle.dump(processor,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def create_model(vocab_size, num_tags):\n",
    "  \n",
    "  model = tf.keras.models.Sequential()\n",
    "  # First layer of neurons\n",
    "  model.add(tf.keras.layers.Dense(50, input_shape=(VOCAB_SIZE,), activation='relu'))\n",
    "  # Second layer\n",
    "  model.add(tf.keras.layers.Dense(35, activation='relu'))\n",
    "# #   #Third Layer\n",
    "#   model.add(tf.keras.layers.Dense(25,activation='relu'))\n",
    "  # final layer=> no. of neurons = no. of labels i.e. 15\n",
    "  model.add(tf.keras.layers.Dense(num_tags, activation='sigmoid'))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(VOCAB_SIZE, num_tags)\n",
    "model.summary()\n",
    "\n",
    "# Train and evaluate the model\n",
    "model.fit(body_train, train_tags, epochs=5, batch_size=128, validation_split=0.1)\n",
    "print('Eval loss/accuracy:{}'.format(\n",
    "  model.evaluate(body_test, test_tags, batch_size=128)))\n",
    "\n",
    "# Export the model to a file\n",
    "model.save('sentiment_analysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(body_train, train_tags, epochs=3, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "model.evaluate(body_test,test_tags,batch_size=128)\n",
    "# testing accuracy is very close to training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "model.save('sentiment_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class CustomModelPrediction(object):\n",
    "\n",
    "  def __init__(self, model, processor):\n",
    "    self._model = model\n",
    "    self._processor = processor\n",
    "  \n",
    "  def predict(self, instances, **kwargs):\n",
    "    preprocessed_data = self._processor.transform_text(instances)\n",
    "    predictions = self._model.predict(preprocessed_data)\n",
    "    return predictions.tolist()\n",
    "\n",
    "  @classmethod\n",
    "  def from_path(cls, model_dir):\n",
    "    import tensorflow.keras as keras\n",
    "    model = keras.models.load_model(\n",
    "      os.path.join(model_dir,'sentiment_model.h5'))\n",
    "    with open(os.path.join(model_dir, 'processor_state.pkl'), 'rb') as f:\n",
    "      processor = pickle.load(f)\n",
    "\n",
    "    return cls(model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_requests=[\n",
    "    \"good for you\" \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction on our local model\n",
    "# from model_prediction import CustomModelPrediction\n",
    "\n",
    "classifier = CustomModelPrediction.from_path('.')\n",
    "results = classifier.predict(test_requests)\n",
    "print(results)\n",
    "for i in range(len(results)):\n",
    "  print('Predicted labels:')\n",
    "  for idx,val in enumerate(results[i]):\n",
    "    if val > 0.1:\n",
    "      print(tag_encoder.classes_[idx])\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
